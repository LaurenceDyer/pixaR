{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b08a0252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import itertools\n",
    "from scipy import ndimage\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import scipy.spatial as sp\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers import SpatialDropout2D, GlobalAveragePooling2D\n",
    "\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score\n",
    "\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3bd301",
   "metadata": {},
   "source": [
    "We have just under 1000 images labelled by the film that they were captured from. All images contain the main (Or a main) character from the film, those being:\n",
    "\n",
    "Cars            :- Lightning McQueen\n",
    "Finding Nemo    :- Nemo\n",
    "Monsters Inc    :- Mike (The Green One)\n",
    "The Incredibles :- Mr. Incredible\n",
    "Up              :- Carl (The Grandpa)\n",
    "Wall-E          :- Eva\n",
    "\n",
    "We chose these characters largely due to their high frequency in the image dataset, created here: \n",
    "            https://github.com/LaurenceDyer/pixaR\n",
    "            \n",
    "We aim to train a CNN model to correctly classify images of the main characters from these films. First, we import the fileset using opencv, then transform it in to an appropriately shaped tensor. This does require resizing the images (And thus losing quite some resolution), also performed via opencv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61f8985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "jpg_files = glob.glob('*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "701bd0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_dict = {0: \"Lightning McQueen\", 1: \"Nemo\", 2: \"Mike\", 3: \"Mr. Incredible\", 4: \"Carl\", 5: \"Eva\"}\n",
    "\n",
    "images = [cv2.imread(file) for file in jpg_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba8e48ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "imageTensor = np.empty((973,528,1280,3), dtype=np.float32)\n",
    "for (k, image) in enumerate(images):\n",
    "    imageTensor[k] = cv2.resize(image, dsize=(1280, 528))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd88b894",
   "metadata": {},
   "source": [
    "We can use string modification to create a label set from the image names of each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e20f7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [re.sub(r\"[0-9]+.jpg\",\"\",x) for x in jpg_files]\n",
    "labels = [re.sub(r\"cars\",\"Lightning McQueen\",x) for x in labels]\n",
    "labels = [re.sub(r\"fnemo\",\"Nemo\",x) for x in labels]\n",
    "labels = [re.sub(r\"monsters\",\"Mike\",x) for x in labels]\n",
    "labels = [re.sub(r\"ti\",\"Mr. Incredible\",x) for x in labels]\n",
    "labels = [re.sub(r\"up\",\"Carl\",x) for x in labels]\n",
    "labels = [re.sub(r\"walle\",\"Eva\",x) for x in labels]\n",
    "\n",
    "labels = np.array(labels).reshape((973,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2f61198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(973, 528, 1280, 3)\n",
      "(973, 1)\n"
     ]
    }
   ],
   "source": [
    "print(imageTensor.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb34afb6",
   "metadata": {},
   "source": [
    "Great! All our tensors now have the correct shape for training. We can use sklearn to split our train/test data.\n",
    "\n",
    "We will also divide our integer values by 255 to normalize them to the range 0-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6312e044",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimageTensor\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m255.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1337\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSplit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:2334\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2329\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _CVIterableWrapper(cv)\n\u001b[0;32m   2331\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cv  \u001b[38;5;66;03m# New style cv objects are passed without any modification\u001b[39;00m\n\u001b[1;32m-> 2334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_test_split\u001b[39m(\n\u001b[0;32m   2335\u001b[0m     \u001b[38;5;241m*\u001b[39marrays,\n\u001b[0;32m   2336\u001b[0m     test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2337\u001b[0m     train_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2338\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2339\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   2340\u001b[0m     stratify\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2341\u001b[0m ):\n\u001b[0;32m   2342\u001b[0m     \u001b[38;5;124;03m\"\"\"Split arrays or matrices into random train and test subsets.\u001b[39;00m\n\u001b[0;32m   2343\u001b[0m \n\u001b[0;32m   2344\u001b[0m \u001b[38;5;124;03m    Quick utility that wraps input validation and\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2424\u001b[0m \u001b[38;5;124;03m    [[0, 1, 2], [3, 4]]\u001b[39;00m\n\u001b[0;32m   2425\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   2426\u001b[0m     n_arrays \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(arrays)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(imageTensor/255.,labels,test_size=0.1,random_state=1337)\n",
    "print(\"Split\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaab3a3f",
   "metadata": {},
   "source": [
    "We reshape our labels into one-hot encoded (N-1) columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768b20ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_dict_inv = {v: k for k, v in char_dict.items()}\n",
    "\n",
    "y_train = np.vectorize(char_dict_inv.get)(y_train)\n",
    "y_test = np.vectorize(char_dict_inv.get)(y_test)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e40afcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test = X_test.reshape(X_test.shape[0],3,528,1280).astype(\"float32\")\n",
    "#X_train = X_train.reshape(X_train.shape[0],3,528,1280).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433b3a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50998312",
   "metadata": {},
   "source": [
    "Now we create our model. We choose to use a CNN because of its proven track record of high-quality modelling in computer vision. Because our dataset is quite small, it is important not to allow for excessive overfitting and as such we will utilise several dropout steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66153568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model():\n",
    "    model = Sequential() \n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(528, 1280, 3), activation = 'relu'))  \n",
    "    model.add(SpatialDropout2D(0.2))\n",
    "    model.add(Conv2D(64, (3, 3), activation = 'relu'))\n",
    "    model.add(SpatialDropout2D(0.2))\n",
    "    model.add(Conv2D(128, (3, 3), activation = 'relu'))\n",
    "    model.add(SpatialDropout2D(0.2))\n",
    "\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    \n",
    "    model.add(Dense(6, activation= 'softmax'))\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam()\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623c28a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cnn_model()\n",
    "\n",
    "history = model.fit(X_train,y_train, validation_split=0.1, epochs=10, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd8ecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_history(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86903ea",
   "metadata": {},
   "source": [
    "The model only achieves a roughly 40% validation accuracy after 10 epochs of training. I'm confident that changes could be made to the neural network to achieve a greater minimal loss, but more training would be required than I have computer hours. I think I computationally bit off a little more than my laptop could chew!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
